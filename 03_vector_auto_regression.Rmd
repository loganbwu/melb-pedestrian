---
title: "Vector Auto Regression"
author: "Logan Wu"
date: "1/16/2019"
output: rmarkdown::github_document
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, cache=T)
library(data.table)
library(sf)
library(leaflet)
library(spdep)                     # easily create weight matrices
library(starma)
library(forecast)
library(ggcorrplot)
library(tidyverse)
library(xts)
library(plotly)
library(vars)
library(urca)
```

## Load data

```{r}
# read in a manageable portion of the dataset for now
raw.ts <- fread("data/Pedestrian_volume__updated_monthly_.csv", nrows=50000)
raw.ts[,Date_Time := as.POSIXct(Date_Time, format="%m/%d/%Y %I:%M:%S %p")]
```

## Process TS data

```{r}
X = raw.ts %>%
  dplyr::select(Date_Time, Sensor_Name, Hourly_Counts) %>%
  spread(key=Sensor_Name, value=Hourly_Counts) %>%
  as.xts(frequency=168) %>%
  as.zoo %>%
  na.contiguous # analysis does not include missing data
names(X) = make.names(names(X))
```

## Perform differencing

Confirm that each sensor series is stationary as we would expect

```{r}
for (i in seq_along(colnames(X))) {
  print(ndiffs(X[,i], test="adf"))
}
```

Test to perform seasonal differencing. Weekly differencing only seems to give the best PACF plot.

```{r, fig.height=3}
data.diff = X %>%
  diff(lag=168)
plot(data.diff[,2])
pacf(data.diff[,2])
```

Test for cointegration (causality).

```{r}
jotest=ca.jo(data.diff[,1:3], type="trace", K=2, ecdet="none", spec="longrun")
summary(jotest)
```

Above is an example of the Johansen co-integration test, using just three series instead of the entire matrix. The $r$ test statistics  reject the null hypothesis that the series are not co-integrated.

Split into train/test sets. Later this can turn into a rolling train/test.

```{r}
frac = 0.8
X.train = head(X, round(nrow(X)*frac))
X.test = tail(X, round(nrow(X)*(1-frac)))
plot(X.train)
title("Original series")

X.train.diff = X.train %>%
  diff(lag=168)
plot(X.train.diff)
title("Seasonally differenced series")
```

Perform vector autoregression. Assume the moving average component is not necessary because the series is stationary. [Useful link](https://stats.stackexchange.com/questions/191851/var-forecasting-methodology)

```{r}
# lag optimisation
VARselect(X.train.diff, lag.max=10, type="both", season=168)

myvar = VAR(X.train.diff, p=1, season=168)

serial.test(myvar, lags.pt=10, type="PT.asymptotic")
# arch.test(myvar, lags.multi=10)
# summary(myvar)
```

Predictions

```{r}
prd <- predict(myvar, n.ahead = nrow(X.test))
```

```{r}
fcst.all = lapply(prd$fcst,
                  function(X) X[,"fcst"]
  )
fcst.all = sapply(names(prd$fcst),
                  function(N) diffinv(fcst.all[[N]],
                                      lag=168,
                                      xi=tail(X.train[,N], 168)
                                      )
                  ) %>%
  as.data.frame %>%
  tail(nrow(X.test)) %>%
  mutate(time = index(X.test)) %>%
  gather(key="location", value="value", -time)
  
X.train.all = X.train %>%
  as.data.frame %>%
  mutate(time = as.POSIXct(rownames(.)),
         key="train") %>%
  gather(key="location", value="value", -time, -key)
X.obs.all = X.test %>%
  as.data.frame %>%
  mutate(time = as.POSIXct(rownames(.)),
         key="obs") %>%
  gather(key="location", value="value", -time, -key)
X.fcst.all = fcst.all %>%
  mutate(key="fcst.var")
X.all = X.train.all %>% rbind(X.obs.all) %>% rbind(X.fcst.all) %>%
  mutate(key=factor(key), location=factor(location)) %>%
  arrange(time)

ggplot(X.all %>% filter(location %in% levels(X.all$location)[1:6]),
       aes(x=time, y=value, color=key)) +
  geom_line(alpha=0.5) +
  facet_wrap(~location, ncol=1) +
  ylim(0, NA)
```

Error quantification

```{r}
MAE = X.obs.all %>%
  full_join(X.fcst.all, by=c("time", "location"), suffix=c(".obs", ".fcst")) %>%
  dplyr::select(time, location, value.obs, value.fcst) %>%
  mutate(AE = abs(value.obs-value.fcst)) %>%
  pull(AE) %>%
  mean
MAE
ME = X.obs.all %>%
  full_join(arfcst, by=c("time", "location"), suffix=c(".obs", ".fcst")) %>%
  dplyr::select(time, location, value.obs, value.fcst) %>%
  mutate(AE = value.obs-value.fcst) %>%
  pull(AE) %>%
  mean()
ME
```

## Comparison with multiple univariate time series

```{r}
myars = lapply(X.train.diff, function(ts) arima(ts, order=c(1,0,0), include.mean=F))

arfcst = sapply(myars, predict, n.ahead=nrow(X.test), se.fit=F) %>%
  as.data.frame
arfcst = sapply(names(arfcst), function(N) diffinv(arfcst[,N],
                                                       lag=168,
                                                       xi=tail(X.train[,N], 168)
                                                       )
                ) %>%
  as.data.frame %>%
  tail(nrow(X.test)) %>%
  mutate(time = index(X.test), key="fcst.ar") %>%
  gather(key="location", value="value", -time, -key)

ggplot(data.frame(VAR=X.fcst.all$value, AR=arfcst$value), aes(x=VAR, y=AR)) +
  geom_point(alpha=0.1)
```

Unlike a univariate AR(1) model, this particular VAR prediction contains negative values.

```{r}
X.all = X.train.all %>% rbind(X.obs.all) %>% rbind(arfcst) %>%
  mutate(key=factor(key), location=factor(location)) %>%
  arrange(time)
ggplot(X.all %>% filter(location %in% levels(X.all$location)[1:6]),
       aes(x=time, y=value, color=key)) +
  geom_line(alpha=0.5) +
  facet_wrap(~location, ncol=1) +
  ylim(0, NA)
error = X.obs.all %>%
  full_join(arfcst, by=c("time", "location"), suffix=c(".obs", ".fcst")) %>%
  dplyr::select(time, location, value.obs, value.fcst) %>%
  mutate(error = value.obs-value.fcst) %>%
  pull(error)
cat("MAE:", mean(abs(error)), "\n")
cat("MEE:", mean(error), "\n")
```

